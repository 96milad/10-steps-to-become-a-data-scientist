{
  "cells": [
    {
      "metadata": {
        "_uuid": "726661972b09b03a31d424ef02a9be0cd284d81b"
      },
      "cell_type": "markdown",
      "source": " # <div style=\"text-align: center\">Tutorial on Ensemble Learning </div>\n <img src='https://data-science-blog.com/wp-content/uploads/2017/12/ensemble-learning-stacking.png' width=400 height=400 >\n### <div style=\"text-align: center\"> Quite Practical and Far from any Theoretical Concepts </div>\n<div style=\"text-align:center\">last update: <b>07/02/2019</b></div>\n\n\n>You are reading **10 Steps to Become a Data Scientist** and are now in the 8th step : \n\n1. [Leren Python](https://www.kaggle.com/mjbahmani/the-data-scientist-s-toolbox-tutorial-1)\n2. [Python Packages](https://www.kaggle.com/mjbahmani/the-data-scientist-s-toolbox-tutorial-2)\n3. [Mathematics and Linear Algebra](https://www.kaggle.com/mjbahmani/linear-algebra-for-data-scientists)\n4. [Programming &amp; Analysis Tools](https://www.kaggle.com/mjbahmani/20-ml-algorithms-15-plot-for-beginners)\n5. [Big Data](https://www.kaggle.com/mjbahmani/a-data-science-framework-for-quora)\n6. [Data visualization](https://www.kaggle.com/mjbahmani/top-5-data-visualization-libraries-tutorial)\n7. [Data Cleaning](https://www.kaggle.com/mjbahmani/machine-learning-workflow-for-house-prices)\n8. <font color=\"red\">You are in the 8th step</font>\n9. [A Comprehensive ML  Workflow with Python](https://www.kaggle.com/mjbahmani/a-comprehensive-ml-workflow-with-python)\n10. [Deep Learning](https://www.kaggle.com/mjbahmani/top-5-deep-learning-frameworks-tutorial)\n\n---------------------------------------------------------------------\nyou can Fork and Run this kernel on <font color=\"red\">Github</font>:\n\n> ###### [ GitHub](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n-------------------------------------------------------------------------------------------------------------\n **I hope you find this kernel helpful and some <font color='red'> UPVOTES</font> would be very much appreciated**\n \n -----------"
    },
    {
      "metadata": {
        "_uuid": "2a01be35950f7a117fc6700e866de3bf5a3ea6b9"
      },
      "cell_type": "markdown",
      "source": "<a id=\"top\"></a> <br>\n## Notebook  Content\n1. [Introduction](#1)\n    1. [Why Ensemble Learning?](#11)\n1. [Ensemble Techniques](#2)\n    1. [what-is-the-difference-between-bagging-and-boosting?](#21)\n1. [XGBoost?](#3)\n    1. [Installing XGBoost ](#31)\n    1. [Matrix Multiplication](#32)\n    1. [Vector-Vector Products](#33)\n    1. [Outer Product of Two Vectors](#34)\n    1. [Matrix-Vector Products](#35)\n    1. [Matrix-Matrix Products](#36)\n1. [Random Forest](#4)\n1. [AdaBoost](#5)\n1. [GBM](#6)\n1. [XGB](#7)\n1. [Light GBM](#8)\n1. [Conclusion](#6)\n1. [References & Credits](#7)"
    },
    {
      "metadata": {
        "_uuid": "b18443661b6d30ffea2150fa74d44d62e14ae952"
      },
      "cell_type": "markdown",
      "source": "<a id=\"1\"></a> <br>\n#  1- Introduction\nIn this kernel, I want to start explorer everything about **Ensemble modeling**. I will run plenty of algorithms on various datasets. I hope you enjoy and give me feedback."
    },
    {
      "metadata": {
        "_uuid": "18e6a0730989363caa069a745b5f3ea8b30766e9"
      },
      "cell_type": "markdown",
      "source": "<a id=\"2\"></a> <br>\n## 2- Import packages"
    },
    {
      "metadata": {
        "_uuid": "5b8aa15d1b11789c38f1dd19d5f06e4be054e525",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport plotly.graph_objs as go\nfrom sklearn import datasets\nimport plotly.plotly as py\nimport seaborn as sns\nimport xgboost as xgb\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\nimport os",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c9e3318fd92fab57b39625950c2e805bc83fa06f"
      },
      "cell_type": "markdown",
      "source": "<a id=\"21\"></a> <br>\n### 2-1 Version"
    },
    {
      "metadata": {
        "_uuid": "49d5cacd5d0aeadd10836b930cdb43e0ed581a60",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "print('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "matplotlib: 2.2.3\nsklearn: 0.20.2\nscipy: 1.1.0\nseaborn: 0.9.0\npandas: 0.23.4\nnumpy: 1.16.0\nPython: 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16) \n[GCC 7.3.0]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ef3610612578ce105a0e8d08693b0ca9e75dcb06"
      },
      "cell_type": "markdown",
      "source": "<a id=\"22\"></a> <br>\n### 2-2 Setup\n\nA few tiny adjustments for better **code readability**"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "3fe93eb33b3c1499d10da8d9840e13ac29cb64d5"
      },
      "cell_type": "code",
      "source": "warnings.filterwarnings('ignore')\nsns.set(color_codes=True)\nplt.style.available\n%matplotlib inline\n%precision 2",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "'%.2f'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e5c5a1da5ce973e4dce69388b76022b5f69e4c16"
      },
      "cell_type": "markdown",
      "source": "<a id=\"23\"></a> <br>\n### 2-3 Data Collection"
    },
    {
      "metadata": {
        "_uuid": "1f4c3ec8ecd51cc0ae810666af8f93d6d1d27aaf",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# import Dataset to play with it\ndataset = pd.read_csv('../input/iris-dataset/Iris.csv')",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a6db3370c16c7e91d1d1624bc88a35cde1f8f141"
      },
      "cell_type": "markdown",
      "source": "**<< Note 1 >>**\n\n* Each row is an observation (also known as : sample, example, instance, record)\n* Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)"
    },
    {
      "metadata": {
        "_uuid": "2488db5250897fc09954c350d5901f3e90c7f855"
      },
      "cell_type": "markdown",
      "source": "<a id=\"3\"></a> <br>\n## 3- What's Ensemble Learning?\nlet us, review some defination on Ensemble Learning:\n\n1. **Ensemble learning** is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem[9]\n1. **Ensemble Learning** is a powerful way to improve the performance of your model. It usually pays off to apply ensemble learning over and above various models you might be building. Time and again, people have used ensemble models in competitions like Kaggle and benefited from it.[6]\n1. **Ensemble methods** are techniques that create multiple models and then combine them to produce improved results. Ensemble methods usually produces more accurate solutions than a single model would.[10]\n<img src='https://hub.packtpub.com/wp-content/uploads/2018/02/ensemble_machine_learning_image_1-600x407.png'  width=400 height=400>\n[img-ref](https://hub.packtpub.com/wp-content/uploads/2018/02/ensemble_machine_learning_image_1-600x407.png)\n\n> <font color=\"red\"><b>Note</b></font>\nEnsemble Learning is a Machine Learning concept in which the idea is to train multiple models using the same learning algorithm. The ensembles take part in a bigger group of methods, called multiclassifiers, where a set of hundreds or thousands of learners with a common objective are fused together to solve the problem.[11]\n\n> <font color=\"red\"><b>Note</b></font>\nThis Kernel assumes a basic understanding of Machine Learning algorithms. I would recommend going through this [**kernel**](https://www.kaggle.com/mjbahmani/a-comprehensive-ml-workflow-with-python)  to familiarize yourself with these concepts.\n"
    },
    {
      "metadata": {
        "_uuid": "7ff16eb2e58c508070cd0ab13a3f49ee61456d62"
      },
      "cell_type": "markdown",
      "source": "<a id=\"31\"></a> <br>\n## 3-1 Why Ensemble Learning?\n1. Difference in population\n1. Difference in hypothesis\n1. Difference in modeling technique\n1. Difference in initial seed\n<br>\n[go to top](#top)"
    },
    {
      "metadata": {
        "_uuid": "aec8b19e1f21c3133c0b6654c8e219620bce2f60"
      },
      "cell_type": "markdown",
      "source": "<a id=\"4\"></a> <br>\n# 4- Ensemble Techniques\nThe goal of any machine learning problem is to find a single model that will best predict our wanted outcome. Rather than making one model and hoping this model is the best/most accurate predictor we can make, ensemble methods take a myriad of models into account, and average those models to produce one final model.[12]\n<img src='https://uploads.toptal.io/blog/image/92062/toptal-blog-image-1454584029018-cffb1b601292e8d328556e355ed4f7e0.jpg' width=300 height=300>\n[img-ref](https://www.toptal.com/machine-learning/ensemble-methods-machine-learning)\n1. Bagging based Ensemble learning\n1. Boosting-based Ensemble learning\n1. Voting based Ensemble learning"
    },
    {
      "metadata": {
        "_uuid": "b7523c62ce012e9abba85f7f14cc49f0e0d11bcf"
      },
      "cell_type": "markdown",
      "source": "<a id=\"41\"></a> <br>\n## 4-1- what-is-the-difference-between-bagging-and-boosting?\n**Bagging**: It is the method to decrease the variance of model by generating additional data for training from your original data set using combinations with repetitions to produce multisets of the same size as your original data.\n\n**Boosting**: It helps to calculate the predict the target variables using different models and then average the result( may be using a weighted average approach).\n<img src='https://www.globalsoftwaresupport.com/wp-content/uploads/2018/02/ds33ggg.png'>\n[img-ref](https://www.globalsoftwaresupport.com/boosting-adaboost-in-machine-learning/)\n<br>\n[go to top](#top)"
    },
    {
      "metadata": {
        "_uuid": "72cc7c7b60a33390a85b16bc34e3b9e424650cdd"
      },
      "cell_type": "markdown",
      "source": "<a id=\"5\"></a> <br>\n## 5- Model Deployment\nIn this section have been applied more than **20 learning algorithms** that play an important rule in your experiences and improve your knowledge in case of ML technique.\n\n> **<< Note 3 >>** : The results shown here may be slightly different for your analysis because, for example, the neural network algorithms use random number generators for fixing the initial value of the weights (starting points) of the neural networks, which often result in obtaining slightly different (local minima) solutions each time you run the analysis. Also note that changing the seed for the random number generator used to create the train, test, and validation samples can change your results.\n<br>\n[go to top](#top)"
    },
    {
      "metadata": {
        "_uuid": "4b7788bbaaace438242d3b2d0d2ed489a91939ce"
      },
      "cell_type": "markdown",
      "source": "<a id=\"51\"></a> <br>\n## 5-1 Families of ML algorithms\nThere are several categories for machine learning algorithms, below are some of these categories:\n* Linear\n    * Linear Regression\n    * Logistic Regression\n    * Support Vector Machines\n* Tree-Based\n    * Decision Tree\n    * Random Forest\n    * GBDT\n* KNN\n* Neural Networks\n\n-----------------------------\nAnd if we  want to categorize ML algorithms with the type of learning, there are below type:\n* Classification\n\n    * k-Nearest \tNeighbors\n    * LinearRegression\n    * SVM\n    * DT \n    * NN\n    \n* clustering\n\n    * K-means\n    * HCA\n    * Expectation Maximization\n    \n* Visualization \tand\tdimensionality \treduction:\n\n    * Principal \tComponent \tAnalysis(PCA)\n    * Kernel PCA\n    * Locally -Linear\tEmbedding \t(LLE)\n    * t-distributed\tStochastic\tNeighbor\tEmbedding \t(t-SNE)\n    \n* Association \trule\tlearning\n\n    * Apriori\n    * Eclat\n* Semisupervised learning\n* Reinforcement Learning\n    * Q-learning\n* Batch learning & Online learning\n* Ensemble  Learning\n\n**<< Note >>**\n> Here is no method which outperforms all others for all tasks\n<br>\n[go to top](#top)"
    },
    {
      "metadata": {
        "_uuid": "8a6fb87ba874c6108aa7266d80c20e161076c40b"
      },
      "cell_type": "markdown",
      "source": "<a id=\"52\"></a> <br>\n## 5-2 XGBoost?\n* **XGBoost** is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data.\n* **XGBoost** is an implementation of gradient boosted decision trees designed for speed and performance.\n* **XGBoost** is short for e**X**treme **G**radient **Boost**ing package."
    },
    {
      "metadata": {
        "_uuid": "787e4b715d1969126ec6757bbb28f9c1fc84e970"
      },
      "cell_type": "markdown",
      "source": "* Speed and performance : Originally written in C++, it is comparatively faster than other ensemble classifiers.\n\n* Core algorithm is parallelizable : Because the core XGBoost algorithm is parallelizable it can harness the power of multi-core computers. It is also parallelizable onto GPU’s and across networks of computers making it feasible to train on very large datasets as well.\n\n* Consistently outperforms other algorithm methods : It has shown better performance on a variety of machine learning benchmark datasets.\n\n* Wide variety of tuning parameters : XGBoost internally has parameters for cross-validation, regularization, user-defined objective functions, missing values, tree parameters, scikit-learn compatible API etc.\n* Win competition On Kaggle : there are a lot of winners on Kaggle that use XGBoost\n<br>\n[go to top](#top)"
    },
    {
      "metadata": {
        "_uuid": "c212686b417d16cea9998ef4446bbd3817b16792"
      },
      "cell_type": "markdown",
      "source": "<a id=\"521\"></a> <br>\n## 5-2-1 Installing XGBoost"
    },
    {
      "metadata": {
        "_uuid": "9efb9808940ca6795af40c18c0e263bf58cfd166"
      },
      "cell_type": "markdown",
      "source": "There is a comprehensive installation guide on the [XGBoost documentation website](http://xgboost.readthedocs.io/en/latest/build.html).\n\n###  XGBoost in R\nIf you are an R user, the best place to get started is the [CRAN page for the xgboost package](https://cran.r-project.org/web/packages/xgboost/index.html).\n\n###  XGBoost in Python\nInstallation instructions are available on the Python section of the XGBoost installation guide.\n\nThe official Python Package Introduction is the best place to start when working with XGBoost in Python.\n\nTo get started quickly, you can type:\n<br>\n[go to top](#top)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a502be5d0750673359705ec54e3414457fd6b21"
      },
      "cell_type": "code",
      "source": "#>sudo pip install xgboost",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "daf9910caba26e071ff560dbdaca079ee148e140"
      },
      "cell_type": "markdown",
      "source": "<a id=\"53\"></a> <br>\n## 5-3 Prepare Features & Targets\nFirst of all seperating the data into dependent(Feature) and independent(Target) variables.\n\n**<< Note 4 >>**\n* X==>>Feature\n* y==>>Target"
    },
    {
      "metadata": {
        "_uuid": "b06cb1191a0f52a904c52a918d1f999536e79bda",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "be230bb37d65624a2be449771bd222620a54f99e"
      },
      "cell_type": "markdown",
      "source": "After loading the data via **pandas**, we should checkout what the content is, description and via the following:\n<br>\n[go to top](#top)"
    },
    {
      "metadata": {
        "_uuid": "ffc339dbf9c8da74194b994930694bd97bb2afbb"
      },
      "cell_type": "markdown",
      "source": "<a id=\"54\"></a> <br>\n## 5-4 RandomForest\nA random forest is a meta estimator that **fits a number of decision tree classifiers** on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n\nThe sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)."
    },
    {
      "metadata": {
        "_uuid": "8ed2305b51c2248a8aa62cf4452632f448e83771",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\nModel=RandomForestClassifier(max_depth=2)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        11\nIris-versicolor       0.93      1.00      0.96        13\n Iris-virginica       1.00      0.83      0.91         6\n\n      micro avg       0.97      0.97      0.97        30\n      macro avg       0.98      0.94      0.96        30\n   weighted avg       0.97      0.97      0.97        30\n\n[[11  0  0]\n [ 0 13  1]\n [ 0  0  5]]\naccuracy is  0.9666666666666667\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1311eb15f2afceed2219faeb859d0d07b7072176"
      },
      "cell_type": "markdown",
      "source": "<a id=\"55\"></a> <br>\n## 5-5 Bagging classifier \nA Bagging classifier is an ensemble **meta-estimator** that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting . If samples are drawn with replacement, then the method is known as Bagging . When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces . Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches .[http://scikit-learn.org]\n<br>\n[go to top](#top)"
    },
    {
      "metadata": {
        "_uuid": "c11c731d3db6c1c81301da85dc158cb7d324c4cb",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import BaggingClassifier\nModel=BaggingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        11\nIris-versicolor       0.93      1.00      0.96        13\n Iris-virginica       1.00      0.83      0.91         6\n\n      micro avg       0.97      0.97      0.97        30\n      macro avg       0.98      0.94      0.96        30\n   weighted avg       0.97      0.97      0.97        30\n\n[[11  0  0]\n [ 0 13  1]\n [ 0  0  5]]\naccuracy is  0.9666666666666667\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c0944bd32424f38906148d96f4b1e6fccfbf97a6"
      },
      "cell_type": "markdown",
      "source": "<a id=\"56\"></a> <br>\n##  5-6 AdaBoost classifier\n\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\nThis class implements the algorithm known as **AdaBoost-SAMME** ."
    },
    {
      "metadata": {
        "_uuid": "938946ee8e017b982c4c06e193d4d13cb7d3fb5f",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import AdaBoostClassifier\nModel=AdaBoostClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        11\nIris-versicolor       0.93      1.00      0.96        13\n Iris-virginica       1.00      0.83      0.91         6\n\n      micro avg       0.97      0.97      0.97        30\n      macro avg       0.98      0.94      0.96        30\n   weighted avg       0.97      0.97      0.97        30\n\n[[11  0  0]\n [ 0 13  1]\n [ 0  0  5]]\naccuracy is  0.9666666666666667\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "9d62842d12731d3eb1d6577c5b35c12c4886c708"
      },
      "cell_type": "markdown",
      "source": "<a id=\"57\"></a> <br>\n## 5-7 Gradient Boosting Classifier\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions."
    },
    {
      "metadata": {
        "_uuid": "863124561c0d1b5995d0b8d3702daa7bc364d6b0",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import GradientBoostingClassifier\nModel=GradientBoostingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        11\nIris-versicolor       0.93      1.00      0.96        13\n Iris-virginica       1.00      0.83      0.91         6\n\n      micro avg       0.97      0.97      0.97        30\n      macro avg       0.98      0.94      0.96        30\n   weighted avg       0.97      0.97      0.97        30\n\n[[11  0  0]\n [ 0 13  1]\n [ 0  0  5]]\naccuracy is  0.9666666666666667\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e89b4494bd78c2d66beeba34a4e320fd8c9dae0c"
      },
      "cell_type": "markdown",
      "source": "<a id=\"58\"></a> <br>\n## 5-8 Linear Discriminant Analysis\nLinear Discriminant Analysis (discriminant_analysis.LinearDiscriminantAnalysis) and Quadratic Discriminant Analysis (discriminant_analysis.QuadraticDiscriminantAnalysis) are two classic classifiers, with, as their names suggest, a **linear and a quadratic decision surface**, respectively.\n\nThese classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no **hyperparameters** to tune."
    },
    {
      "metadata": {
        "_uuid": "0796cd9f1c902345df605b7557a9c3ff686e35a9",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nModel=LinearDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        11\nIris-versicolor       1.00      1.00      1.00        13\n Iris-virginica       1.00      1.00      1.00         6\n\n      micro avg       1.00      1.00      1.00        30\n      macro avg       1.00      1.00      1.00        30\n   weighted avg       1.00      1.00      1.00        30\n\n[[11  0  0]\n [ 0 13  0]\n [ 0  0  6]]\naccuracy is  1.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "296137970fc94fa4a4eb4185cb5fa952b1985c57"
      },
      "cell_type": "markdown",
      "source": "<a id=\"59\"></a> <br>\n## 5-9 Quadratic Discriminant Analysis\nA classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.\n\nThe model fits a **Gaussian** density to each class."
    },
    {
      "metadata": {
        "_uuid": "5f521d19f295b8e8f24f5715e93b1c45e9a6bce3",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nModel=QuadraticDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                 precision    recall  f1-score   support\n\n    Iris-setosa       1.00      1.00      1.00        11\nIris-versicolor       1.00      1.00      1.00        13\n Iris-virginica       1.00      1.00      1.00         6\n\n      micro avg       1.00      1.00      1.00        30\n      macro avg       1.00      1.00      1.00        30\n   weighted avg       1.00      1.00      1.00        30\n\n[[11  0  0]\n [ 0 13  0]\n [ 0  0  6]]\naccuracy is  1.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "5246353e23d70ac5b76f2f0000da0fb575aad4c1"
      },
      "cell_type": "markdown",
      "source": "<a id=\"510\"></a> <br>\n##  5-10 XGBoost\nFinally see how to perform XGBoost"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1396f3c01a68cebf0ad533d37d167b78853684e8",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1df71cb9d0303e4e3f5b7f5ddbf82447745fe171",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "82460e2a3e4d12ea56bb89661362ed245a86b2e1",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd22dbaa135ca53b6e082b0a20a67d339fc61b90",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.datasets import dump_svmlight_file\n\ndump_svmlight_file(X_train, y_train, 'dtrain.svm', zero_based=True)\ndump_svmlight_file(X_test, y_test, 'dtest.svm', zero_based=True)\ndtrain_svm = xgb.DMatrix('dtrain.svm')\ndtest_svm = xgb.DMatrix('dtest.svm')",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[10:29:02] 120x4 matrix with 480 entries loaded from dtrain.svm\n[10:29:02] 30x4 matrix with 120 entries loaded from dtest.svm\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "174cdc50c4f13271d1f20099b8c50f53020c8bd3",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "param = {\n    'max_depth': 3,  # the maximum depth of each tree\n    'eta': 0.3,  # the training step for each iteration\n    'silent': 1,  # logging mode - quiet\n    'objective': 'multi:softprob',  # error evaluation for multiclass training\n    'num_class': 3}  # the number of classes that exist in this datset\nnum_round = 20  # the number of training iterations",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0581372dc2d832490e67cc3e6dc8773c24da2a46",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "bst = xgb.train(param, dtrain, num_round)",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "286c1f024331c206414a5447bce7394799e2a9a6",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "bst.dump_model('dump.raw.txt')",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "176133bf0e133d48a223d6c0892834e8864357c0",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "preds = bst.predict(dtest)",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "36eb5f69380105fa41ae6e41aa522fe512b49731",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "best_preds = np.asarray([np.argmax(line) for line in preds])",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "78a2dd79efdf874f57a2b4c14c56654ff4864bfc"
      },
      "cell_type": "markdown",
      "source": "Determine the precision of this prediction:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72fa1b9a9f9ecdc42f723162df7ece0da7f519e9",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import precision_score\n\nprint (precision_score(y_test, best_preds, average='macro'))",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "1.0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ad034a77267412573d68eccac15c7c08d1b3e7f8"
      },
      "cell_type": "markdown",
      "source": "## 5-11 Extremely Randomized Trees\nIn extremely randomized trees[13]"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "3b6cf0686831f9f4607d3393e67a96e815948e01"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nX, y = make_blobs(n_samples=10000, n_features=10, centers=100,\nrandom_state=0)\n\nclf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\nrandom_state=0)\nscores = cross_val_score(clf, X, y, cv=5)\nscores.mean()   ",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "0.9823000000000001"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "afc2a360fedd783e5e9d7bbc975c9c6f06a2ee72"
      },
      "cell_type": "markdown",
      "source": "<a id=\"6\"></a> <br>\n# 6-Conclusion\n* That XGBoost is a library for developing fast and high performance gradient boosting tree models.\n* That XGBoost is achieving the best performance on a range of difficult machine learning tasks.\n<br>\n[go to top](#top)"
    },
    {
      "metadata": {
        "_uuid": "b132163ee07917a0ab100b93f6ed5545ce0de45d"
      },
      "cell_type": "markdown",
      "source": "you can follow me on:\n> ###### [ GitHub](https://github.com/mjbahmani)\n> ###### [Kaggle](https://www.kaggle.com/mjbahmani/)\n\n **I hope you find this kernel helpful and some upvotes would be very much appreciated**\n "
    },
    {
      "metadata": {
        "_uuid": "5719a5ba111b65b20b53d538281ac773eb14471a"
      },
      "cell_type": "markdown",
      "source": "<a id=\"10\"></a> <br>\n# 7-References & Credits"
    },
    {
      "metadata": {
        "_uuid": "aab5b3d8cb417250dc6baa081a579106900effba"
      },
      "cell_type": "markdown",
      "source": "1. [datacamp](https://www.datacamp.com/community/tutorials/xgboost-in-python)\n1. [Xgboost presentation](https://www.oreilly.com/library/view/data-science-from/9781491901410/ch04.html)\n1. [machinelearningmastery](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)\n1. [analyticsvidhya](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n1. [Github](https://github.com/mjbahmani)\n1. [analyticsvidhya](https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/)\n1. [ensemble-learning-python](https://www.datacamp.com/community/tutorials/ensemble-learning-python)\n1. [image-header-reference](https://data-science-blog.com/blog/2017/12/03/ensemble-learning/)\n1. [scholarpedia](http://www.scholarpedia.org/article/Ensemble_learning)\n1. [toptal](https://www.toptal.com/machine-learning/ensemble-methods-machine-learning)\n1. [quantdare](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/)\n1. [towardsdatascience](https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f)\n1. [scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html)"
    },
    {
      "metadata": {
        "_uuid": "905a9a2ba1f3acee4e8f85df99cfb0cc9c924b28"
      },
      "cell_type": "markdown",
      "source": ">If you have read the notebook, you can follow next steps: [Course Home Page](https://www.kaggle.com/mjbahmani/10-steps-to-become-a-data-scientist)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}